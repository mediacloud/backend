---
name: MediaWords

### database settings. at least one database connection must be defined. the
### main "production" database should be the first one below.
database:

    # production
    - label : "database"
      type  : "pg"
      host  : "localhost"
      port  : 15432
      db    : "mediacloud"
      user  : "mediacloud"
      pass  : "Omomric2"

### Amazon S3 connection settings
amazon_s3:

    ### Bucket for storing downloads
    downloads:
        access_key_id      : "AKIAJTUSOK6MW743NFZQ"
        secret_access_key  : "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"
        bucket_name        : "mediacloud-downloads-backup"
        directory_name     : "downloads"

    ### Bucket for testing
    #test:
        #access_key_id      : "AKIAIOSFODNN7EXAMPLE"
        #secret_access_key  : "wJalrXUtnFEMI/K7MDENG/bPxRfiCYzEXAMPLEKEY"
        #bucket_name        : "mediacloud_test"

        ### An unique random string will be appended to the directory name
        #directory_name     : "downloads_test"

## Job manager (MediaCloud::JobManager) configuration
job_manager:

    ## When uncommented, will use RabbitMQ as job broker
    rabbitmq:

        ## RabbitMQ client configuration
        ## (both workers and clients will use this key)
        client:

            ## Connection credentials
            hostname: "localhost"
            port: 5673     # not the default 5672
            username: "mediacloud"
            password: "mediacloud"
            vhost: "/mediacloud"
            timeout: 60

        ## RabbitMQ server configuration
        ## (rabbitmq_wrapper.sh will use this for starting up an instance of
        ## RabbitMQ)
        server:

            ## To disable your very own RabbitMQ instance managed by Supervisord,
            ## set the below to "false" (without quotes). Default is "true".
            enabled: true

            ## Host to listen to. You can set the above parameter to an empty string
            ## so that RabbitMQ will accept connections from anywhere; however, it is
            ## highly advised use to secure channels (e.g. a SSH tunnel) to make RabbitMQ
            ## accessible from "outside" instead. Default is "127.0.0.1".
            listen: "127.0.0.1"

            ## Port to use for RabbitMQ. Default port for vendor-provided RabbitMQ
            ## deployments is 5672, but Media Cloud runs its own RabbitMQ instance via
            ## Supervisord. Default is 5673.
            port: 5673     # not the default 5672

            ## Node name
            node_name: "mediacloud@localhost"

            ## User credentials and vhost to create upon start (instead of "guest")
            username: "mediacloud"
            password: "mediacloud"
            vhost: "/mediacloud"

### Supervisor (supervisord) configuration
supervisor:

    ### The log directory for child process logs (absolute or relative to Media
    ### Cloud's root; must already exist)
    childlogdir: "data/supervisor_logs/"

    # If set to true, do not autostart any programs unless the program is explicitly
    # set to be autostarted below
    start_no_supervisor_programs: true

    # configure supervisor settings for mediacloud background daemons here.
    # the defaults should work for a small dev setup, but you will want to increase
    # numprocs for some daemons depending on load.  you can also set some daemons
    # not to autostart -- for instance you might want to change crawler.autostart
    # to "false" (without quotes) to prevent the crawler from starting automatically on a dev machine.
    programs:

        crawler:
            numprocs: 5
            autostart: false
            autorestart: false

        extract_and_vector:
            numprocs: 5
            autostart: false
            autorestart: true

        solr_standalone:
            autostart: false

        rescrape_media:
            numprocs: 2
            autostart: false

        create_missing_partitions:
            autostart: false

        rabbitmq:
            autostart: false


        # other configurable supervisor programs
        #create_missing_partitions
        #purge_object_caches
        #facebook_fetch_story_stats
        #rabbitmq
        #rescrape_media
        #topic_mine
        #topic_snapshot

        # Standalone Solr instance
        #solr_standalone

        # Solr cluster: ZooKeeper instance
        #solr_cluster_zookeeper

        # Solr cluster: Solr shards
        # (Don't set "numprocs" here, adjust "cluster_shard_count" / "local_shard_count" instead.)
        #solr_shard

### Solr server, when running as a Supervisor service
supervisor_solr:

    ### Standalone Solr instance
    standalone:

        # JVM heap size (-Xmx)
        jvm_heap_size: "256m"

    ### Solr cluster
    cluster:

        ### ZooKeeper instance
        zookeeper:

            ### Address to bind to
            listen: "0.0.0.0"

            ### Port to listen to
            port: 9983

        ### Solr shards
        shards:

            # Total number of local shards
            local_shard_count: 2

            # Total number of shards across the cluster ("numShards")
            cluster_shard_count: 2

            # JVM heap size for a single shard (-Xmx)
            jvm_heap_size: "256m"

            # ZooKeeper host + port to connect shards to
            zookeeper_host: "localhost"
            zookeeper_port: 9983

associated_press:
    apikey: "1jbq3nem1wg8buig43kfnpdfkr"

### SimilarWeb API
similarweb:

    ### API key, costs money, see at https://developer.similarweb.com/
    api_key: ""


### CLIFF annotator
cliff:

    ### Enable CLIFF processing
    ### If enabled, CLIFF processing will happen after every "content"
    ### download extraction
    enabled: false

    ### Annotator URL (text parsing endpoint), e.g. "http://localhost:8080/cliff-2.4.1/parse/text"
    annotator_url: ""

    ### CLIFF version tag, e.g. "cliff_clavin_v2.4.1"; will be added under
    ### "geocoder_version" tag set
    cliff_version_tag: "cliff_clavin_v2.4.1"

    ### CLIFF geographical names tag set, e.g. "cliff_geonames";
    ### tags with names such as "geonames_<countryGeoNameId>" will be added
    ### under this tag set
    cliff_geonames_tag_set: "cliff_geonames"

    ### CLIFF organizations tag set, e.g. "cliff_organizations"; tags with
    ### names of organizations such as "United Nations" will be added under
    ### this tag set
    cliff_organizations_tag_set: "cliff_organizations"

    ### CLIFF people tag set, e.g. "cliff_people"; tags with names of people
    ### such as "Einstein" will be added under this tag set
    cliff_people_tag_set: "cliff_people"

### NYTLabels annotator
nytlabels:

    ### Enable NYTLabels processing
    ### If enabled, NYTLabels processing will happen after every "content"
    ### download extraction
    enabled: false

    ### Annotator URL (text parsing endpoint), e.g. "http://localhost/predict.json"
    annotator_url: ""

    ### NYTLabels version tag, e.g. "nyt_labeller_v1.0.0"; will be added under
    ### "geocoder_version" tag set
    nytlabels_version_tag: "nyt_labeller_v1.0.0"

    ### NYTLabels tag set, e.g. "nyt_labels"; tags with names such as
    ### "hurricane" will be added under this tag set
    nytlabels_labels_tag_set: "nyt_labels"

### Facebook API
### (see doc/README.facebook_api.markdown)
facebook:

    ### Enable Facebook processing
    enabled: false

    ## App ID
    app_id: ""

    ## App Secret
    app_secret: ""

    ## Request timeout
    #timeout: 60

twitter:
   consumer_key: "fWKCpGie8lfa1PtUswnZunjrU"
   consumer_secret: "g3FY3SPbaky4GZqw0oYF3EU930KUY0qEkspaMJ2LUVPnXfyj1U"
   access_token: "38100863-3ZzLK4MLj3EkBJrqdNglUydNDFYjBDT1Jb0YYLIfC"
   access_token_secret: "tBZrtkxIjEgd4AX1RtquzMJLy1FbAK6LmzKBliTFAcHud"


# key to fetch tweets from crimson hexagon.  necessary for topic tweets as implements in FetchTopicTweets.pm
#crimson_hexagon:
#   key: ""

### Univision.com feed credentials
#univision:
    ### Client ID
    #client_id: 83db02e1cba58c43d01116c50014913b47fa473b

    ### Client Secret (Secret Key)
    #client_secret: 7187037755de2dd77451f491d46b103b86fbcf79

### Email configuration
mail:

    # "From:" email address that is being set in emails sent by Media Cloud
    from_address: "noreply@mediacloud.org"

    ### (optional) SMTP configuration
    smtp:

        ### SMTP host
        host: "localhost"

        ### SMTP port
        port: 25

        ### Use STARTTLS? If you enable that, you probably want to change the port to 587.
        starttls: false

        ### (optional) SMTP login credentials
        username: ""
        password: ""

### everything below is optional.  the system should work out of the box without
### touching any of these other than calais_key for tagging

#session:
    #expires: 3600

    ### directory where web app sessions are stored.  default to $homedir/tmp
    #storage: "~/tmp/mediawords-session"

## Uncomment and fill in to use Google Analytics
#google_analytics:
#      account: "<ACOUNT>"
#      domainname: "<DOMAIN>"

mediawords:
    ### defaults to http://$hostname:$port/.
    #base_url: "http://your.mediacloud.server/and/path"

    ### Directory in which various kinds of data (logs, etc.) is being stored
    #data_dir: "<bindir>/../data"

    ### HTTP user agent and the email address of the owner of the bot
    user_agent: "mediawords bot (http://cyber.law.harvard.edu)"
    owner: "mediawords@cyber.law.harvard.edu"

    ### Domains that might need HTTP auth credentials to work
    #crawler_authenticated_domains:
        #- domain: "ap.org"
        #  user: "username"
        #  password: "password"

    ### Uncomment one or more storage methods to store downloads in.
    ### Default is "postgresql" which stores downloads directly in the
    ### PostgreSQL database.
    ###
    ### Very short downloads will be stored directly in the database, under
    ### "downloads.path"
    ###
    ### The path of the last download storage method listed below will be
    ### stored in "downloads.path" database column.
    download_storage_locations:
        ### store downloads in the PostgreSQL database, "raw_downloads" table
        #- postgresql
        ### store downloads in Amazon S3
        - amazon_s3

    ### Read all non-inline ("content") downloads from S3
    read_all_downloads_from_s3 : false

    ### Uncomment to fallback PostgreSQL downloads to Amazon S3 (if download
    ### doesn't exist in PostgreSQL storage, S3 will be tried instead)
    fallback_postgresql_downloads_to_s3 : false

    ### Enable local Amazon S3 download caching?
    cache_s3_downloads : false

    #controls the maximum time SQL queries can run for -- time is in ms
    #uncomment to enable a 10 minute timeout
    #db_statement_timeout: "600000"

    # Uncommit to speed up slow queries by setting the Postgresql work_mem parameter to this value
    # By default the initial Postgresql value of work_mem is used
    # large_work_mem: "3GB"

    # An experiment parameter to dump stack traces in error message even if not in debug mode
    # NOTE: may leak DB passwords and is not to be use in production
    always_show_stack_traces: false

    # reCAPTCHA public key (used to prevent brute-force in the password reset form)
    # The default value was set up for http://127.0.0.1 and is a global key (should work across all domains)
    recaptcha_public_key: "6LfEVt0SAAAAAFwQI0pOZ1bTHgDTpQcMeQY6VLd_"

    # reCAPTCHA private key (used to prevent brute-force in the password reset form)
    # The default value was set up for http://127.0.0.1 and is a global key (should work across all domains)
    recaptcha_private_key: "6LfEVt0SAAAAABmI-8IJmx4g93eNcSeyeCxvLMs2"

    #uncomment to make the public homepage the default start page
    default_home_page: "admin/media/list"

    # downloads id under which to strip all non-ascii characters
    #ascii_hack_downloads_id: 123456789

    # settings for parallel_get()
    web_store_num_parallel: 10
    web_store_timeout: 90
    web_store_per_domain_timeout: 1

    # Fail all HTTP requests that match the following pattern
    # blacklist_url_pattern: "^https?://[^/]*some-website.com"

    # tablespace in which to create temporary tables -- defaults to the postgres default
    # temporary_table_tablespace: temporary_tablespace

    # url for solr word counting url.  if this is set, fetch word counts from a remote server
    # using this url; otherwise, generate word counts locally
    # solr_wc_url: http://localhost/api/v2/wc

    # mc api key for appending to sol_wc_url for fetching remote word counts
    # solr_wc_key: FOO

    # URLs for Solr queries; include multiple to make Media Cloud choose a random URL from
    # the list for each Solr query
    solr_url:

        # Standalone Solr instance...
        - http://localhost:17983/solr

        # ...or SolrCloud shards
        #- http://127.0.0.1:7981/solr
        #- http://127.0.0.1:7982/solr
        #- http://127.0.0.1:7983/solr
        #- http://127.0.0.1:7984/solr
        #- http://127.0.0.1:7985/solr
        #- http://127.0.0.1:7986/solr
        #- http://127.0.0.1:7987/solr
        #- http://127.0.0.1:7988/solr

    # Solr importer configuration
    solr_import:
        # Stories to import into Solr on a single run
        max_queued_stories: 100000

    # set to "true" (without quotes) to skip requirement to run on the correct
    # database schema version
    # ignore_schema_version: true

    # increment wc_cache_version to invalidate existing cache
    # wc_cache_version: 1

    # list of emails to which to send all topic alerts
    # topic_alert_emails:
    #     - topicupdates@mediacloud.org
    #     - slackupdates@mediacloud.org
