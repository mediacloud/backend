Manual testing script for basic mediacloud functionality.

TEST

* run the following to create a fresh database

script/run_with_carton.sh script/mediawords_create_db.pl 

VERIFY

* no errors during db creation (NOTICEs are okay)

TEST

* add a user using the following command:

./script/run_with_carton.sh ./script/mediawords_manage_users.pl \
         --action=add \
         --email=jdoe@cyber.law.harvard.edu \
         --full_name="John Doe" \
         --notes="test" \
         --roles="admin" \
         --password="testtest"

VERIFY

* script returns with no errors

TEST

* run the following command to start the various backend daemons:

./supervisor/supervisord.sh

* run the following command to verify that supervisord has started the background jobs:

./supervisor/supervisorctl.sh

VERIFY

* verify that the following scripts are list as RUNNING:

gearman:add_default_feeds_00     RUNNING
gearman:add_default_feeds_01     RUNNING
gearman:extract_and_vector_00    RUNNING
gearman:extract_and_vector_01    RUNNING
gearman:search_stories_00        RUNNING
gearman:search_stories_01        RUNNING
mc:crawler                       RUNNING
mc:process_story_subsets         RUNNING

TEST

* run the below to start the web app

script/run_server_with_carton.sh 
 
* bring up mediacloud admin site in browser (http://localhost:3000/)
* login with the above test user
* click add media
* in the large box enter the following:

nytimes.com collection:newspapers
washingtonpost.com collection:newspapers
orlandosentinel.com collection:newspapers
boingboing.net collection:blogs
redstate.com collection:blogs;collection:conservative_blogs

* click the Add Media button

VERIFY

* login is successful
* all pages load without errors
* each of the above sources appears in the media list, each with the associated tags

TEST

* wait about ten minutes for the background feed scraping jobs to complete their work

VERIFY

* verify that each media source has at least one feed associated with it

TEST

* restart the crawler by running the following command:

supervisor/supervisorctl.sh restart mc:crawler

* then watch the output of the crawler by running this command:

supervisor/supervisorctl.sh tail -f mc:crawler stderr

* wait a few minutes until you see this starting repeating and hit ctl-c to exit the tail

refill queued downloads ...
adding missing downloads
provide downloads: 0 downloads

VERIFY

* verify that the crawler is downloading content, with log entries that look something like this:

fetcher 0 get downloads_id: '240' http://feeds.washingtonpost.com/rss/sports/dcunited processing complete [ 0.515178 / 2.190159 ]
fetcher 3 get downloads_id: '56' http://www.nytimes.com/services/xml/rss/nyt/Soccer.xml starting
fetcher 0 get downloads_id: '241' http://feeds.washingtonpost.com/rss/world/war-zones starting
fetcher 2 completed handle response: http://feeds.washingtonpost.com/rss/politics/polling
fetcher 2 get downloads_id: '249' http://feeds.washingtonpost.com/rss/politics/polling processing complete [ 0.55863 / 1.927545 ]
fetcher 4 get downloads_id: '55' http://www.nytimes.com/services/xml/rss/nyt/Movies.xml fetched
fetcher 4 starting handle response: http://www.nytimes.com/services/xml/rss/nyt/Movies.xml
refill queued downloads ...
fetcher 2 get downloads_id: '250' http://feeds.washingtonpost.com/rss/rss_rosenwald-md starting
fetcher 1 completed handle response: http://feeds.washingtonpost.com/rss/local/obituaries

TEST

* restart and tail the crawler again:

supervisor/supervisorctl.sh restart mc:crawler
supervisor/supervisorctl.sh tail -f mc:crawler stderr

* hit ctl-c after a couple of minutes

VERIFY

* verify that the crawler is downloading content with entries something like this:

fetcher 4 get downloads_id: '365' http://well.blogs.nytimes.com/2013/10/31/a-fat-dad-halloween-hide-the-candy/ fetched
fetcher 4 starting handle response: http://well.blogs.nytimes.com/2013/10/31/a-fat-dad-halloween-hide-the-candy/
fetcher 2 get downloads_id: '363' http://boss.blogs.nytimes.com/2013/10/29/how-a-burned-out-owner-got-back-into-the-game/ fetched
fetcher 2 starting handle response: http://boss.blogs.nytimes.com/2013/10/29/how-a-burned-out-owner-got-back-into-the-game/
fetcher 4 get downloads_id: '365' http://well.blogs.nytimes.com/2013/10/31/a-fat-dad-halloween-hide-the-candy/ processing complete [ 2.155058 / 0.009765 ]
fetcher 2 get downloads_id: '363' http://boss.blogs.nytimes.com/2013/10/29/how-a-burned-out-owner-got-back-into-the-game/ processing complete [ 2.128335 / 0.005992 ]
fetcher 2 get downloads_id: '520' http://feedproxy.google.com/~r/orlandosentinel/~3/NSx0pz94TGQ/story01.htm starting
fetcher 4 get downloads_id: '518' http://feedproxy.google.com/~r/orlandosentinel/~3/f2SGiWpXPLg/story01.htm starting
fetcher 3 get downloads_id: '361' http://boss.blogs.nytimes.com/2013/10/30/today-in-small-business-adwords-or-bing/ fetched
fetcher 3 starting handle response: http://boss.blogs.nytimes.com/2013/10/30/today-in-small-business-adwords-or-bing/

TEST

* load http://localhost:3000/admin/downloads/view/<id>, where <id> is one of the single quoted numbers from the tail above

VERIFY

* verify that the html of the given download displays in the browser

TEST

* tail the extractor:

supervisor/supervisorctl.sh tail -f mc:extract_and_vector stderr

* hit ctl-c after about a minute

VERIFY

* verify that the extractor is extracting content with log entries that look like this:

[2, 2] extract: 3030 2708 http://www.nytimes.com/reuters/2013/10/30/sports/golf/30reuters-golf-pga.html?partner=rss&emc=rss
[3, 3] extract: 3419 3097 http://feeds.washingtonpost.com/c/34656/f/636604/s/32eee77c/sc/1/l/0L0Swashingtonpost0N0Clifestyle0Cstyle0Cask0Eamy0Ekids0Edo0Eend0Erun0Earound0Edads0Ewife0C20A130C10A0C250C23bbf310A0E31f70E11e30E9c680E1cf643210A30A0A0Istory0Bhtml0Dwprss0Frss0Iadvice/story01.htm
[1, 1] extract: 3381 3059 http://www.nytimes.com/2013/11/01/world/europe/secular-turkish-government-permits-religious-symbol.html?partner=rss&emc=rss
[6, 6] extract: 2997 2674 http://www.nytimes.com/2013/11/03/books/review/how-has-twitter-changed-the-role-of-the-literary-critic.html?partner=rss&emc=rss
[5, 5] extract: 3217 2895 http://www.nytimes.com/2013/10/31/business/in-dispute-over-a-song-marvin-gayes-family-files-a-countersuit.html?partner=rss&emc=rss
[2, 2] extract: 3050 2728 http://www.nytimes.com/aponline/2013/10/28/sports/golf/ap-glf-lpga-ko.html?partner=rss&emc=rss
[9, 9] extract: 3113 2791 http://www.nytimes.com/2013/08/16/science/space/nasas-kepler-mended-but-may-never-fully-recover.html?partner=rss&emc=rss

TEST

* load http://localhost:3000/admin/stories/view/<id> where <id> is the second of the id in one of the numbers above (eg. 2708)

VERIFY

* verify that reasonable extracted text appears in the Extracted Text field

TEST

* Click Dashboards
* Click Create New Dashboard

* Fill out the fields as follows:

name: mydash
Start Date : 2000-01-01
End Date: 2020-01-01

* Click on Create

VERIFY

* 'mydash' appears in list of dashboards

TEST

* Click mediasets on mydash row
* Click Create New Media Set
* Fill out as follows

Name: Newspapers
Description: Newspapers
Collection Tag: collection:newspapers

* Click Go

* Repeat twice with the following info:

Name: blogs
Description: blogs
Collection Tag: collection:blogs

Name: conservative_blogs
Description: conservative_blogs
Collection Tag: collection:conservative_blogs

VERIFY

* all three media sets appear in the list with the associated tags

TEST

* click on 'Search Media'
* click on the 'feeds' link for boingboing
* click on the first feed in the boingboing list
* click on the bottom story in the list of feeds
* click on the 'view download' link on the story page

VERIFY

* verify that there is a list of stories on the feed page
* verify that the extracted text from the story appears after the second *** 
  in the Extracted Text section of the story page
* verify that the raw text of the download appears on the 'view download' page


TEST

* run the following command and wait until it exits (should be a couple of minutes at most):

./script/run_with_carton.sh ./script/mediawords_update_aggregate_vectors.pl 

VERIFY

* Click ‘Dashboards’
* Click the ‘view’ link for the 'mydash' dashboard
* Verify that a word cloud appears
* Select the current week and click submit
* Select each media set and click submit for each
* Verify that a word cloud appears for each media set
* Click a word in the word cloud and verify that sentences appear
* In the Media Sources box in the Query section start typing “New York Times” and verify that autocomplete works

